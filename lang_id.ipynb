{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2THL5TU5_0TN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0256a0-7d4f-4dd3-872d-c8794aaecd0d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "main_directory = os.path.join('drive/MyDrive/Colab Notebooks/projects/language_id')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "check_dir(main_directory)\n",
        "%cd drive/MyDrive/Colab Notebooks/projects/language_id"
      ],
      "metadata": {
        "id": "u-RSHJX4__6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37f7950-fc05-4243-d2b1-c39cce66ae03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/projects/language_id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NamazovMN/Language-Identification.git"
      ],
      "metadata": {
        "id": "KeG9rYJ0AEZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959b068b-a431-405d-84b6-f70574c89957"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Language-Identification'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 185 (delta 48), reused 98 (delta 47), pack-reused 84\u001b[K\n",
            "Receiving objects: 100% (185/185), 1.17 MiB | 6.64 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Language-Identification"
      ],
      "metadata": {
        "id": "aypErHPjAG9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48375e10-a6ab-42eb-ecba-dd2dbc2afd67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "zuGcyOUKAIdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc0690b-dc39-4dfd-a465-fafca5c384be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas~=1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: torch~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.1+cu118)\n",
            "Collecting datasets~=2.13.1 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm~=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn~=1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: nltk~=3.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: matplotlib~=3.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: seaborn~=0.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.5.3->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.0.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch~=2.0.0->-r requirements.txt (line 2)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch~=2.0.0->-r requirements.txt (line 2)) (16.0.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets~=2.13.1->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (2.27.1)\n",
            "Collecting xxhash (from datasets~=2.13.1->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets~=2.13.1->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets~=2.13.1->-r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.13.1->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.2.1->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->-r requirements.txt (line 6)) (8.1.6)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->-r requirements.txt (line 6)) (2022.10.31)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.7.0->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.13.1->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas~=1.5.3->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.13.1->-r requirements.txt (line 3)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.13.1->-r requirements.txt (line 3)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.13.1->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.0.0->-r requirements.txt (line 2)) (2.1.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets~=2.13.1->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.4 multiprocess-0.70.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usk7xBEHLwvc",
        "outputId": "db09edf9-6f3d-40fd-f965-bbdccedccb1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guidelines\n",
        "Before starting the training please choose such experiment number that does not exist. Github Repo includes results files but not models. Loading is done based on those files.\n",
        "\n",
        "Easy fix: Remove all folders, they will be generated automatically for chosen experiment."
      ],
      "metadata": {
        "id": "cGYkaVL0HyOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --train --experiment_num 2 --epochs 3"
      ],
      "metadata": {
        "id": "lCrtngYbAM-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d591e0-42a4-4e64-9bdb-fc988020ae90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading readme: 100% 4.99k/4.99k [00:00<00:00, 16.2MB/s]\n",
            "Downloading and preparing dataset csv/papluca--language-identification to /root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/12.0M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 52.2k/12.0M [00:00<00:29, 406kB/s]\u001b[A\n",
            "Downloading data:   2% 296k/12.0M [00:00<00:09, 1.27MB/s]\u001b[A\n",
            "Downloading data:  10% 1.25M/12.0M [00:00<00:02, 4.07MB/s]\u001b[A\n",
            "Downloading data: 100% 12.0M/12.0M [00:00<00:00, 19.5MB/s]\n",
            "Downloading data files:  33% 1/3 [00:01<00:03,  1.72s/it]\n",
            "Downloading data:   0% 0.00/1.71M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   7% 117k/1.71M [00:00<00:01, 970kB/s]\u001b[A\n",
            "Downloading data: 100% 1.71M/1.71M [00:00<00:00, 5.50MB/s]\n",
            "Downloading data files:  67% 2/3 [00:02<00:01,  1.11s/it]\n",
            "Downloading data:   0% 0.00/1.69M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  19% 312k/1.69M [00:00<00:00, 2.63MB/s]\u001b[A\n",
            "Downloading data: 100% 1.69M/1.69M [00:00<00:00, 6.89MB/s]\n",
            "Downloading data files: 100% 3/3 [00:03<00:00,  1.05s/it]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1267.16it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 320.11it/s]\n",
            "Epoch: 1 Train Loss:  1.0490 Train Accuracy:  0.7262: 100% 2188/2188 [01:57<00:00, 18.67it/s]\n",
            "Validation Loss:  0.3758; Validation Accuracy:  0.8983: 100% 313/313 [00:01<00:00, 255.00it/s]\n",
            "Train and Validation results were added to data for 1\n",
            "Model and optimizer parameters were saved successfully for 1\n",
            "<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>\n",
            "Epoch: 2 Train Loss:  0.1902 Train Accuracy:  0.9473: 100% 2188/2188 [01:56<00:00, 18.72it/s]\n",
            "Validation Loss:  0.3051; Validation Accuracy:  0.9230: 100% 313/313 [00:01<00:00, 224.45it/s]\n",
            "Train and Validation results were added to data for 2\n",
            "Model and optimizer parameters were saved successfully for 2\n",
            "<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>\n",
            "Epoch: 3 Train Loss:  0.1061 Train Accuracy:  0.9701: 100% 2188/2188 [01:57<00:00, 18.69it/s]\n",
            "Validation Loss:  0.2594; Validation Accuracy:  0.9366: 100% 313/313 [00:01<00:00, 222.79it/s]\n",
            "Train and Validation results were added to data for 3\n",
            "Model and optimizer parameters were saved successfully for 3\n",
            "<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>\n",
            "Test Loss:  0.2481; Test Accuracy:  0.9360: 100% 313/313 [00:01<00:00, 238.96it/s]\n",
            "Test Results after 3 epochs: Test Loss:  0.2481, Test Accuracy:  0.9360,f1 score:  0.9371\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(1000x1200)\n",
            "Figure(1000x1200)\n",
            "Figure(1000x1200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained experiemnt number was chosen, which is 3. Epoch coice was set to that number and the load_best was not set. Thus the chosen epoch will be loaded for inference."
      ],
      "metadata": {
        "id": "3C4_yrj4PGIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --epoch_choice 3 --experiment_num 2"
      ],
      "metadata": {
        "id": "1aVVYXj6FeqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568314e3-d154-441f-d31d-5da28bcc96f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 34.49it/s]\n",
            "Model and optimizer were loaded successfully for epoch 3 wrt user choice\n",
            "Please provide your text: \n",
            "Meios de transporte no Brasil\n",
            "Given text is in portuguese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same experiment number was chosen. This time we set load_best, so that the best choice will be chosen. Notice that we did not specify wrt what the best choice is decided. Thus, by default f1_score was chosen for inference."
      ],
      "metadata": {
        "id": "gm80rK8BPaDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --load_best --experiment_num 2"
      ],
      "metadata": {
        "id": "y9j4r7Z7Kgvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa10cadc-c00f-408c-9f5a-9ec914ba05b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 681.85it/s]\n",
            "Model was chosen according to f1_score\n",
            "Model and optimizer were loaded successfully for epoch 3 wrt user choice\n",
            "Please provide your text: \n",
            "Meios de transporte no Brasil\n",
            "Given text is in portuguese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example is for resume training. Resume training must be set with train parameter, otherwise train session will not be activated. resume_training is activated within training session. Remember number of epochs was set 3 previously, now we set it to 5. Thus, 2 more epochs are expected to be passed."
      ],
      "metadata": {
        "id": "jTXvyJCTPzan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --train --resume_training --epochs 5 --experiment_num 2"
      ],
      "metadata": {
        "id": "DFCgwAO3Limt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab801a3-c57e-470f-e91b-4ee2a145f367"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 646.70it/s]\n",
            "No specific epoch was chosen. Thus, the latest epoch parameters will be loaded\n",
            "Model and optimizer were loaded successfully for epoch 3 wrt user choice\n",
            "Epoch: 4 Train Loss:  0.0702 Train Accuracy:  0.9803: 100% 2188/2188 [01:57<00:00, 18.59it/s]\n",
            "Validation Loss:  0.2494; Validation Accuracy:  0.9434: 100% 313/313 [00:01<00:00, 250.67it/s]\n",
            "Train and Validation results were added to data for 4\n",
            "Model and optimizer parameters were saved successfully for 4\n",
            "<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>\n",
            "Epoch: 5 Train Loss:  0.0520 Train Accuracy:  0.9859: 100% 2188/2188 [01:57<00:00, 18.70it/s]\n",
            "Validation Loss:  0.2059; Validation Accuracy:  0.9575: 100% 313/313 [00:01<00:00, 250.89it/s]\n",
            "Train and Validation results were added to data for 5\n",
            "Model and optimizer parameters were saved successfully for 5\n",
            "<<<<<<<<<<<<<<<<<<<< >>>>>>>>>>>>>>>>>>>>\n",
            "Test Loss:  0.1895; Test Accuracy:  0.9579: 100% 313/313 [00:01<00:00, 215.02it/s]\n",
            "Test Results after 5 epochs: Test Loss:  0.1895, Test Accuracy:  0.9579,f1 score:  0.9582\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Figure(1000x1200)\n",
            "Figure(1000x1200)\n",
            "Figure(1000x1200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example for failure: input_data folder is empty. That is why, we get such error. If we check the message, it says:\n",
        "\n",
        "*Make sure there is input_text.txt file in the input_data folder!*"
      ],
      "metadata": {
        "id": "9ME9ous0Qd2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --from_file --load_best --experiment 2"
      ],
      "metadata": {
        "id": "xixjLkJIQSZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bac807-ee87-427e-d6d7-7fc8ecfb32bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 362.00it/s]\n",
            "Model was chosen according to f1_score\n",
            "Model and optimizer were loaded successfully for epoch 5 wrt user choice\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification/src/main.py\", line 93, in <module>\n",
            "    __main__()\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification/src/main.py\", line 88, in __main__\n",
            "    inference.infer_process()\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification/src/inference.py\", line 154, in infer_process\n",
            "    self.infer_with_file()\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/projects/language_id/Language-Identification/src/inference.py\", line 143, in infer_with_file\n",
            "    raise FileNotFoundError('Make sure there is input_text.txt file in the input_data folder!')\n",
            "FileNotFoundError: Make sure there is input_text.txt file in the input_data folder!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we uploaded input_text.txt with some text. Let's see which language the text was written:"
      ],
      "metadata": {
        "id": "BeOWU870RdQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --from_file --load_best --experiment 2"
      ],
      "metadata": {
        "id": "Exb17zYOQ1tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9865012-1086-46d0-8994-a87ef3e3d1cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 691.82it/s]\n",
            "Model was chosen according to f1_score\n",
            "Model and optimizer were loaded successfully for epoch 5 wrt user choice\n",
            "Given text is in turkish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example is for the specific case: when epoch choice and load best were set together. And let's make specific choice dev_loss this time, which stands for validation loss. In this case, the epoch with the minimum validation loss will be chosen and we will get the following warning of redundant entry, epoch_choice. Notice that load_best has priority!\n",
        "\n",
        "Note: Please be sure you changed  choice option as follows in the main.py, when inference object is initialized:\n",
        "\n",
        "Update: This code line was corrected in the github source, so that you can run it easily without any modification.\n",
        "from:\n",
        "\n",
        "`choice=project_parameters['choice'],`\n",
        "\n",
        "into\n",
        "    \n",
        "`choice=project_parameters['load_choice'],`\n",
        "\n",
        "We missed that part when we added files to github.\n",
        "Rest parameters will remain as in previous example.\n",
        "\n"
      ],
      "metadata": {
        "id": "q8F4L27SR8YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --infer --from_file --load_best --experiment 2 --epoch_choice 4 --load_choice dev_loss"
      ],
      "metadata": {
        "id": "4ivyuxMWR45c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28450d26-1cd6-4486-9a13-5ba8ea5b9bf0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/papluca___csv/papluca--language-identification-b5901b71d851feea/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100% 3/3 [00:00<00:00, 461.49it/s]\n",
            "Model was chosen according to dev_loss\n",
            "Model and optimizer were loaded successfully for epoch 5 wrt user choice\n",
            "Given text is in turkish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I hope you will enjoy the playground!\n",
        "\n",
        "Best Regards,\n",
        "\n",
        "Mahammad Namazov"
      ],
      "metadata": {
        "id": "vCosnE6KT_-Z"
      }
    }
  ]
}